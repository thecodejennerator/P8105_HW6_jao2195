Homework Assignemnt 6 <br> (P8105_HW6_jao2195)
================
Jennifer Osei <br>
Saturday December 02, 2023

################################################################################ 

### Problem 0

################################################################################ 

Problem 0 This “problem” focuses on structure of your submission,
especially the use git and GitHub for reproducibility, R Projects to
organize your work, R Markdown to write reproducible reports, relative
paths to load data from local files, and reasonable naming structures
for your files.

To that end: -Create a public GitHub repo + local R Project; we suggest
naming this repo / directory p8105_hw6_YOURUNI (e.g. p8105_hw6_ajg2202
for Jeff), but that’s not required

-Create a single .Rmd file named p8105_hw6_YOURUNI.Rmd that renders to
github_document create a subdirectory to store the local data files used
in the assignment, and use relative paths to access these data files
submit a link to your repo via Courseworks

Your solutions to Problems 1 and 2 should be implemented in your .Rmd
file, and your git commit history should reflect the process you used to
solve these Problems.

For this Problem, we will assess adherence to the instructions above
regarding repo structure, git commit history, and whether we are able to
knit your .Rmd to ensure that your work is reproducible. Adherence to
appropriate styling and clarity of code will be assessed in Problems 1+
using the style rubric.

This homework includes figures; the readability of your embedded plots
(e.g. font sizes, axis labels, titles) will be assessed in Problems 1+.

``` r
#Needed libaries
library(tidyverse)
library(readr)
library(ggplot2)
library(modelr)
```

################################################################################ 

### Problem 1

################################################################################ 

Background: The Washington Post has gathered data on homicides in 50
large U.S. cities and made the data available through a GitHub
repository here. You can read their accompanying article here.

**1A** <br> **1aQ.** Create a city_state variable (e.g. “Baltimore,
MD”), and a binary variable indicating whether the homicide is solved.
Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t
report victim race. Also omit Tulsa, AL – this is a data entry mistake.
For this problem, limit your analysis those for whom victim_race is
white or black. Be sure that victim_age is numeric.

**1aA.** In the data cleaning code below we create a `city_state`
variable, change `victim_age` to numeric, modifiy victim_race to have
categories white and non-white, with white as the reference category,
and create a `resolution` variable indicating whether the homicide is
solved. Lastly, we filtered out the following cities: Tulsa, AL; Dallas,
TX; Phoenix, AZ; and Kansas City, MO; and we retained only the variables
`city_state`, `resolution`, `victim_age`, `victim_sex`, and
`victim_race`.

**1aC.**

``` r
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (8): uid, victim_last, victim_first, victim_race, victim_sex, city, stat...
    ## dbl (4): reported_date, victim_age, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**1B** <br> **1bQ.** For the city of Baltimore, MD, use the glm function
to fit a logistic regression with resolved vs unresolved as the outcome
and victim age, sex and race as predictors. Save the output of glm as an
R object; apply the broom::tidy to this object; and obtain the estimate
and confidence interval of the adjusted odds ratio for solving homicides
comparing male victims to female victims keeping all other variables
fixed.

**1bA.** Next we fit a logistic regression model using only data from
Baltimore, MD. We model `resolved` as the outcome and `victim_age`,
`victim_sex`, and `victim_race` as predictors. We save the output as
`baltimore_glm` so that we can apply `broom::tidy` to this object and
obtain the estimate and confidence interval of the adjusted odds ratio
for solving homicides comparing non-white victims to white victims.

**1bC.**

``` r
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

|    OR | OR_CI_lower | OR_CI_upper |
|------:|------------:|------------:|
| 0.426 |       0.325 |       0.558 |

**1C** <br> **1cQ.** Now run glm for each of the cities in your dataset,
and extract the adjusted odds ratio (and CI) for solving homicides
comparing male victims to female victims. Do this within a “tidy”
pipeline, making use of purrr::map, list columns, and unnest as
necessary to create a dataframe with estimated ORs and CIs for each
city.

**1cA.** Below, by incorporating `nest()`, `map()`, and `unnest()` into
the preceding Baltimore-specific code, we fit a model for each of the
cities, and extract the adjusted odds ratio (and CI) for solving
homicides comparing non-white victims to white victims. We show the
first 5 rows of the resulting dataframe of model results.

**1cC.**

``` r
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

| city_state      |    OR | OR_CI_lower | OR_CI_upper |
|:----------------|------:|------------:|------------:|
| Albuquerque, NM | 1.767 |       0.831 |       3.761 |
| Atlanta, GA     | 1.000 |       0.684 |       1.463 |
| Baltimore, MD   | 0.426 |       0.325 |       0.558 |
| Baton Rouge, LA | 0.381 |       0.209 |       0.695 |
| Birmingham, AL  | 0.870 |       0.574 |       1.318 |

**1D** <br> **1dQ.** Create a plot that shows the estimated ORs and CIs
for each city. Organize cities according to estimated OR, and comment on
the plot.

**1dA.** Below we generate a plot of the estimated ORs and CIs for each
city, ordered by magnitude of the OR from smallest to largest. From this
plot we see that most cities have odds ratios that are smaller than 1,
suggesting that crimes with male victims have smaller odds of resolution
compared to crimes with female victims after adjusting for victim age
and race. This disparity is strongest in New yrok. In roughly half of
these cities, confidence intervals are narrow and do not contain 1,
suggesting a significant difference in resolution rates by sex after
adjustment for victim age and race.

**1dC.**

``` r
Model_Results = model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Added ggsave to organize files and store problem output plots in a folder. 
ggsave("plots/Problem_1_Model_Results.png", 
        plot = Model_Results, device = "png", width = 10, height = 6, dpi = 300)

Model_Results
```

![](P8105_HW6_jao2195_files/figure-gfm/q1_plot-1.png)<!-- -->

################################################################################ 

### Problem 2

################################################################################ 

For this problem, we’ll use the Central Park weather data similar to
data we’ve seen elsewhere. The code chunk below (adapted from the course
website) will download these data.

``` r
#Downloading data needed for Problem 2: Central Park Weather Data.
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

The boostrap is helpful when you’d like to perform inference for a
parameter / value / summary that doesn’t have an easy-to-write-down
distribution in the usual repeated sampling framework. We’ll focus on a
simple linear regression with tmax as the response with tmin and prcp as
the predictors, and are interested in the distribution of two quantities
estimated from these data:

**Quantity 1: r^2**

**Quantity 2: log(β̂ 1∗β̂ 2)**

Use 5000 bootstrap samples and, for each bootstrap sample, produce
estimates of these two quantities. Plot the distribution of your
estimates, and describe these in words. Using the 5000 bootstrap
estimates, identify the 2.5% and 97.5% quantiles to provide a 95%
confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1) . Note: broom::glance() is
helpful for extracting r̂ 2 from a fitted regression, and broom::tidy()
(with some additional wrangling) should help in computing log(β̂ 1∗β̂ 2) .

################################################################################ 

### Problem 3

################################################################################ 

################################################################################ 

### Problem 4

################################################################################ 
