Homework Assignemnt 6 <br> (P8105_HW6_jao2195)
================
Jennifer Osei <br>
Saturday December 02, 2023

################################################################################ 

### Problem 0

################################################################################ 

Problem 0 This “problem” focuses on structure of your submission,
especially the use git and GitHub for reproducibility, R Projects to
organize your work, R Markdown to write reproducible reports, relative
paths to load data from local files, and reasonable naming structures
for your files.

To that end: -Create a public GitHub repo + local R Project; we suggest
naming this repo / directory p8105_hw6_YOURUNI (e.g. p8105_hw6_ajg2202
for Jeff), but that’s not required

-Create a single .Rmd file named p8105_hw6_YOURUNI.Rmd that renders to
github_document create a subdirectory to store the local data files used
in the assignment, and use relative paths to access these data files
submit a link to your repo via Courseworks

Your solutions to Problems 1 and 2 should be implemented in your .Rmd
file, and your git commit history should reflect the process you used to
solve these Problems.

For this Problem, we will assess adherence to the instructions above
regarding repo structure, git commit history, and whether we are able to
knit your .Rmd to ensure that your work is reproducible. Adherence to
appropriate styling and clarity of code will be assessed in Problems 1+
using the style rubric.

This homework includes figures; the readability of your embedded plots
(e.g. font sizes, axis labels, titles) will be assessed in Problems 1+.

``` r
#Needed libaries

# install.packages("tidyverse")
# install.packages("modelr")
# install.packages("boot")

library(tidyverse)
library(modelr)
library(boot)
```

################################################################################ 

### Problem 1

################################################################################ 

Background: The Washington Post has gathered data on homicides in 50
large U.S. cities and made the data available through a GitHub
repository here. You can read their accompanying article here.

**1A** **1aQ** Create a city_state variable (e.g. “Baltimore, MD”), and
a binary variable indicating whether the homicide is solved. Omit cities
Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim
race. Also omit Tulsa, AL – this is a data entry mistake. For this
problem, limit your analysis those for whom victim_race is white or
black. Be sure that victim_age is numeric.

**1aC**

``` r
homicide_df =
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |>
  filter(victim_race %in% c("White", "Black")) |>
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |>
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (8): uid, victim_last, victim_first, victim_race, victim_sex, city, stat...
    ## dbl (4): reported_date, victim_age, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

**1aA** In the data cleaning code below we create a `city_state`
variable, change `victim_age` to numeric, modifiy victim_race to have
categories white and non-white, with white as the reference category,
and create a `resolution` variable indicating whether the homicide is
solved. Lastly, we filtered out the following cities: Tulsa, AL; Dallas,
TX; Phoenix, AZ; and Kansas City, MO; and we retained only the variables
`city_state`, `resolution`, `victim_age`, `victim_sex`, and
`victim_race`.

**1B** **1bQ** For the city of Baltimore, MD, use the glm function to
fit a logistic regression with resolved vs unresolved as the outcome and
victim age, sex and race as predictors. Save the output of glm as an R
object; apply the broom::tidy to this object; and obtain the estimate
and confidence interval of the adjusted odds ratio for solving homicides
comparing male victims to female victims keeping all other variables
fixed.

**1bA.** Next we fit a logistic regression model using only data from
Baltimore, MD. We model `resolved` as the outcome and `victim_age`,
`victim_sex`, and `victim_race` as predictors. We save the output as
`baltimore_glm` so that we can apply `broom::tidy` to this object and
obtain the estimate and confidence interval of the adjusted odds ratio
for solving homicides comparing non-white victims to white victims.

**1bC**

``` r
baltimore_glm =
  filter(homicide_df, city_state == "Baltimore, MD") |>
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |>
  broom::tidy() |>
  mutate(
    OR = exp(estimate),
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |>
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

|    OR | OR_CI_lower | OR_CI_upper |
|------:|------------:|------------:|
| 0.426 |       0.325 |       0.558 |

**1C** **1cQ** Now run glm for each of the cities in your dataset, and
extract the adjusted odds ratio (and CI) for solving homicides comparing
male victims to female victims. Do this within a “tidy” pipeline, making
use of purrr::map, list columns, and unnest as necessary to create a
dataframe with estimated ORs and CIs for each city.

**1cA** Below, by incorporating `nest()`, `map()`, and `unnest()` into
the preceding Baltimore-specific code, we fit a model for each of the
cities, and extract the adjusted odds ratio (and CI) for solving
homicides comparing non-white victims to white victims. We show the
first 5 rows of the resulting dataframe of model results.

**1cC**

``` r
model_results =
  homicide_df %>%
  nest(data = -city_state) %>%
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race,
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) %>%
  select(-models, -data) %>%
  unnest(cols = tidy_models) %>%
  mutate(
    OR = exp(estimate),
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) %>%
  filter(term == "victim_sexMale") |>
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |>
  knitr::kable(digits = 3)
```

| city_state      |    OR | OR_CI_lower | OR_CI_upper |
|:----------------|------:|------------:|------------:|
| Albuquerque, NM | 1.767 |       0.831 |       3.761 |
| Atlanta, GA     | 1.000 |       0.684 |       1.463 |
| Baltimore, MD   | 0.426 |       0.325 |       0.558 |
| Baton Rouge, LA | 0.381 |       0.209 |       0.695 |
| Birmingham, AL  | 0.870 |       0.574 |       1.318 |

**1D** **1dQ** Create a plot that shows the estimated ORs and CIs for
each city. Organize cities according to estimated OR, and comment on the
plot.

**1dA** Below we generate a plot of the estimated ORs and CIs for each
city, ordered by magnitude of the OR from smallest to largest. From this
plot we see that most cities have odds ratios that are smaller than 1,
suggesting that crimes with male victims have smaller odds of resolution
compared to crimes with female victims after adjusting for victim age
and race. This disparity is strongest in New yrok. In roughly half of
these cities, confidence intervals are narrow and do not contain 1,
suggesting a significant difference in resolution rates by sex after
adjustment for victim age and race.

**1dC**

``` r
Model_Results = model_results |>
  mutate(city_state = fct_reorder(city_state, OR)) |>
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Added ggsave to organize files and store problem output plots in a folder.
ggsave("plots/Problem_1_Model_Results.png",
        plot = Model_Results, device = "png", width = 10, height = 6, dpi = 300)

Model_Results
```

![](P8105_HW6_jao2195_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

################################################################################ 

### Problem 2

################################################################################ 

**2** <br>

**2a** <br> For this problem, we’ll use the Central Park weather data
similar to data we’ve seen elsewhere. The code chunk below (adapted from
the course website) will download these data.

``` r
#2a.Downloading data needed for Problem 2: Central Park Weather Data.
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

**2b** <br> The bootstrap is helpful when you’d like to perform
inference for a parameter / value / summary that doesn’t have an
easy-to-write-down distribution in the usual repeated sampling
framework. We’ll focus on a simple linear regression with tmax as the
response with tmin and prcp as the predictors, and are interested in the
distribution of two quantities estimated from these data:

**Quantity 1: r^2** \<- Interested in producing r^2 distribution. <br>
**Quantity 2: log(β̂ 1xβ̂ 2)**\<- Interested in producing log(β̂1xβ̂ 2)
distribution as well.

1)  Use 5000 bootstrap samples and, for each bootstrap sample, 2)
    produce estimates of these two quantities. 3) Plot the distribution
    of your estimates, and describe these in words. 4) Using the 5000
    bootstrap estimates, identify the 2.5% and 97.5% quantiles to
    provide a 95% confidence interval for r̂^2 and log(β̂ 0xβ̂ 1)

**Note:** broom::glance() is helpful for extracting r̂^2 from a fitted
regression, and broom::tidy() (with some additional wrangling) should
help in computing log(β̂ 1xβ̂ 2)

``` r
##Bootstrap Method 

#weather_df (Dataset)
#tmax (Outcome/Response (Y)) 
#tmin & prcp (Predictor/Explanatory (X))

# Define the function to extract the quantities of interest from the linear regression model
boot_function <- function(weather_df, indices) {
  bootstrap_data <- weather_df[indices, ]   # Subseting the data using the bootstrap indices
  model <- lm(tmax ~ tmin + prcp, data = bootstrap_data)   # Fit the linear regression model 
  r_squared <- summary(model)$r.squared # Quantity 1: R-squared
  log_beta_product <- log(coef(model)["tmin"] * coef(model)["prcp"])# Quantity 2: log(beta1 * beta2)
  return(c(r_squared, log_beta_product))
}

# Set the seed for reproducibility
set.seed(123)

# Number of bootstrap samples
num_bootstrap_samples <- 5000

# Perform the bootstrap
bootstrap_results <- boot(data = weather_df, statistic = boot_function, R = num_bootstrap_samples)

# Plot the distributions of the quantities of interest
par(mfrow = c(1, 2))  # Create a 1x2 layout for two plots
hist(bootstrap_results$t[, 1], main = "Distribution of R-squared", xlab = "R-squared")
hist(bootstrap_results$t[, 2], main = "Distribution of log(beta1 * beta2)", xlab = "log(beta1 * beta2)")
```

![](P8105_HW6_jao2195_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

``` r
# Describe the distributions
# You can use summary(bootstrap_results) to get summary statistics

# Calculate 95% confidence intervals
conf_intervals <- boot.ci(bootstrap_results, type = "bca", index = c(1, 2))
conf_intervals
```

    ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
    ## Based on 1609 bootstrap replicates
    ## 
    ## CALL : 
    ## boot.ci(boot.out = bootstrap_results, type = "bca", index = c(1, 
    ##     2))
    ## 
    ## Intervals : 
    ## Level       BCa          
    ## 95%   ( 0.8759,  0.9355 )  
    ## Calculations and Intervals on Original Scale
    ## Warning : BCa Intervals used Extreme Quantiles
    ## Some BCa intervals may be unstable

################################################################################ 

### Problem 3

################################################################################ 

In this problem, you will analyze data gathered to understand the
effects of several variables on a child’s birthweight. This dataset,
available here, consists of roughly 4000 children and includes the
following variables:

`babysex`: baby’s sex (male = 1, female = 2) <br> `bhead`: baby’s head
circumference at birth (centimeters) <br> `blength`: baby’s length at
birth (centimeteres) <br> `bwt`: baby’s birth weight (grams) <br>
`delwt`: mother’s weight at delivery (pounds) <br> `fincome`: family
monthly income (in hundreds, rounded) <br> `frace`: father’s race (1 =
White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
<br> `gaweeks`: gestational age in weeks <br> `malform`: presence of
malformations that could affect weight (0 = absent, 1 = present) <br>
`menarche`: mother’s age at menarche (years) <br> `mheigth`: mother’s
height (inches) <br> `momage`: mother’s age at delivery (years) <br>
`mrace`: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto
Rican, 8 = Other) <br> `parity`: number of live births prior to this
pregnancy <br> `pnumlbw`: previous number of low birth weight babies
<br> `pnumgsa`: number of prior small for gestational age babies <br>
`ppbmi`: mother’s pre-pregnancy BMI <br> `ppwt`: mother’s pre-pregnancy
weight (pounds) <br> `smoken`: average number of cigarettes smoked per
day during pregnancy <br> `wtgain`: mother’s weight gain during
pregnancy (pounds) <br>

**3** **3A** Load and clean the data for regression analysis <br>
(i.e. convert numeric to factor where appropriate, check for missing
data, etc.).

**3B** Model 1: Propose a regression model for birthweight. This model
may be based on a hypothesized structure for the factors that underly
birthweight, on a data-driven model-building process, or a combination
of the two. Describe your modeling process and show a plot of model
residuals against fitted values – use add_predictions and add_residuals
in making this plot.

Compare your model (Model 1) to two others (Model 2 and Model 3 below) :

Model 2: One using length at birth and gestational age as predictors
(main effects only). Model 3: One using head circumference, length, sex,
and all interactions (including the three-way interaction) between
these.

Make this comparison in terms of the cross-validated prediction error;
use crossv_mc and functions in purrr as appropriate. Note that although
we expect your model to be reasonable, model building itself is not a
main idea of the course and we don’t necessarily expect your model to be
“optimal”.
